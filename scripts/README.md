# Scripts
We provides all experiments' scripts mentioned in our paper. We sort them by folder name:
1. ModelConvert: for model download and conversion
2. Profiler: for the latecny data profiling
3. FakeCalib: generate fake calibration data (for smoothQuant, TODO(remove it later))
4. OptimizerScripts: for the optimizer to run for best strategy (main experiments here)
5. RunningScripts: scripts to run the plan generated by the optimizer (main experiments here)
6. AccuracyPPL: scripts measure the accuracy and PPL of the quantized model (main experiments here)
4. Others: other data appeared in paper

You should first prepare (1-3) before running (4-6).

## ModelConvert
We require first convert model weight into finer granularity. Enter the folder using `cd`.

### DownloadModel
1. Prepare your `.env` under the modelConvert folder like this:
```bash
    HF_TOKEN=<>
    REPO_ID='facebook/opt-125m'
    CACHE_DIR='/data/llms/'
```
2. Specify the following entry
    1. HF_TOKEN: your hf token, required for some model like llama2. In this exp we only support BLOOM and OPT.
    2. REPO_ID: e.g. 'facebook/opt-125m'
    3. CACHE_DIR: By default, please place to '/data/llms/'
3. Run `python3 model_download.py` to download your model
4. Run `bash convert_weight.sh` to convert you weights.

## Profiler
You should enter the profile folder using `cd`, and then the profiled result will be located on the right place.
- You can try `bash profile_test.sh` to see how it works

We provides our pre-profiled results
- [Latency Result](https://connecthkuhk-my.sharepoint.com/:f:/g/personal/juntaozh_connect_hku_hk/ErGI0vzdj6JIjJSimenJ1M0BMiMfsxI4cZzuKwc5psLybw?e=suVaSx).
- [Latency Prepost Profiled Result](https://connecthkuhk-my.sharepoint.com/:f:/g/personal/juntaozh_connect_hku_hk/Evat-lrrmh9GiMvbh2uHSpQBcWrk9bWpDyxFHd8tLfpHgg?e=rwXgJV)
- [Comm Cost Model](https://connecthkuhk-my.sharepoint.com/:f:/g/personal/juntaozh_connect_hku_hk/EtdY8oIJpR9Fnh1mmvj0OJ0B2jXkGf8HpsGLaTrDEDoTyQ?e=XQ1y5j)
If you want to profile by your own, run
-  `profile_all.sh` for both
    - `profile.sh` for latency
    - `profile_concat.sh` for prepost result
-  `profile_comm.sh` for communication, you need to run on both devices with correct setups.


## FakeCalib
Fake calibration data is required to do quantization. Go to fakeCalib and run `bash gen_fake.sh`
- We will remove this part later

As a result, sepcify fakeCalib path by setting env: `CALIB_ROOT_FOLDER`

## Perf Mode
If you are only concerned with performance rather than accuracy, or if you simply want to benchmark the performance in some cases, we recommend using the following commands:

```bash
os.environ['SET_DECODERS_META'] = "1"
os.environ['PERF_MODE'] = "0"
```
1. First option create empty decoder[None] for each device then load weight accordingly.
2. The second option tells QPipe to randomly initialize the weights. By doing so, you can greatly reduce the CPU usage when loading the model and quickly obtain benchmark results.
3. However, in perf model, the performance may not consistent to the weight-loadded case. 

Enable perf-mode by passing
`python xx.py --perf-mode`



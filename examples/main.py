
from qllm.utils import batch_encode_plus, get_model_size_cuda,  to_device_recursive, greedy_processor
import lptorch
import torch
import copy
import os 
from shaq import init_random_seed
import pickle
from qllm.tp import utils as tp_utils
from qllm.utils.argparser import model_sample_gen_argparser
import qllm.utils as qllm_utils
from qllm.models import qllm_load_pretrained_from_size, init_tokenizer,  create_empty_model
from qllm.models import qllm_load_pretrained_from_size, get_decoder_layer_nums
from qllm.scheduler import DSScheduler
# import sharding strategies helper
from example_utils import create_uniform_sharding_strategies
from utils import create_uniform_sharding_strategies, parse_args
if __name__ == '__main__':
    args = parse_args()
    bitwidth = args.bitwidth
    if type(bitwidth) is not int and bitwidth.isnumeric():
        args.bitwidth = int(bitwidth)
    
    if args.sample_run:
        # uniform partition and uniform bitwidth samples
        # but also not use the hybrid batch size
        num_shards = args.num_shards
        prefill_bs = args.bs_token // num_shards
        decoder_bss = [args.bs_token // num_shards] * num_shards
    
        bs_token = args.bs_token
        # set by default
        num_tokens_to_generate = args.num_tokens_to_generate
        max_tokens_to_generate = args.max_tokens_to_generate
        prompt_length = args.prompt_length
        # sharing strategy
        bitwidth = args.bitwidth
        decoder_layer_nums = get_decoder_layer_nums(args.model_name, args.model_size)
        sharding_strategy = create_uniform_sharding_strategies(num_shards, decoder_layer_nums, bitwidth)
    else:
        # load the strategy generated by the SHAQ
        method = args.method
        sol_file = f"{args.strat_file_name}.pkl"
        root_dir = os.environ['ROOT_DIR']
        strat_folder = f'{root_dir}/scripts/part_strategy'
        sols_path = f'{strat_folder}/{sol_file}'
        sols = pickle.load(open(sols_path, "rb"))
        # handle the old version
        if 'model_name' in sols:
            model_name = sols['model_name']
            model_size = sols['model_size']
            args.model_name = model_name
            args.model_size = model_size
        num_tokens_to_generate = sols['mu_n']
        max_tokens_to_generate = sols['n']
        bs_token = sols['gloabl_bz'] # how many sentence in a batch
        assert args.method in sols, f"no {args.method} in {sols_path}"
        # get sols info
        sol = sols[args.method]
        sharding_strategy = sol['use_plan']
        prefill_bs = sol['prefill_bz']
        decoder_bss = sol['bz_decode_bss']
        prompt_length = args.prompt_length if sols.get('prompt_length') is None else sols['prompt_length']

    # test case
    model_name = args.model_name
    model_size = args.model_size
    '''
        Performance mode
        - when you only concerns the performance rather than accuracy
    '''
    # set env
    tokenizer, key = init_tokenizer(model_name, model_size)
    if args.perf_mode:
        os.environ['SET_DECODERS_META'] = "1"
        os.environ['PERF_MODE'] = "1"
        loaded_llm_cpu = create_empty_model(model_name, model_size)
    else:
        load_in_np = os.environ.get('LOAD_IN_NP', '0') == '1'
        if load_in_np:
            # specify the weight folder
            os.environ['NP_WEIGHT_FOLDER'] = os.environ.get('NP_WEIGHT_FOLDER', '/data/llms/converted_weights_np') + f"/{model_name}_{model_size}"
            # load model
            os.environ['SET_DECODERS_META'] = "1"
            loaded_llm_cpu = create_empty_model(model_name, model_size)
            qllm_utils.load_np_weight_opt_non_layer(os.environ['NP_WEIGHT_FOLDER'], loaded_llm_cpu)
        else:
            # case when CPU memory is abundant, direcly load the converted weight
            data_llms_folder = os.environ.get('TRANSFORMERS_CACHE', '/data/llms')
            target_storage_folder = f'{data_llms_folder}/converted_weights'
            # load model 
            if model_name == 'bloom':
                qllm_model, tokenizer, key = qllm_load_pretrained_from_size(model_name, model_size)
            elif model_name == 'opt':
                # use converted weight
                path = os.path.join(target_storage_folder, f"{model_name}_{model_size}")
                if not os.path.exists(path):
                    raise ValueError("Please run weight_convert.py first")
                qllm_model, tokenizer, key = qllm_load_pretrained_from_size(model_name, model_size, target_storage_folder=target_storage_folder)
            loaded_llm_cpu = qllm_model

    # load the fake calibration data
    caliber = lptorch.inner_caliber
    caliber.set_fake() 
    file_path = os.path.abspath(__file__) if os.environ.get('CALIB_ROOT_FOLDER', None) is None else os.environ.get('CALIB_ROOT_FOLDER')
    calib_file_name = f'fake_calib_{model_name}_{model_size}.pkl'
    calib_file_path = os.path.join(os.path.dirname(file_path), 'fake_calib', calib_file_name)
    caliber.load_fake_calib_data(calib_file_path)

    # warmup for performance
    warmup_tokens = args.warmup_tokens
    # hybrid micro-batch scheduler
    chunk_size = len(decoder_bss)
    ds_scheduler = DSScheduler(prefill_bs, decoder_bss)
    # print(prefill_bs, decoder_bss)
    # configs
    infer_configs = (bs_token, prompt_length, num_tokens_to_generate, chunk_size)
    loaded_llm_cpu._verify_shard_strategy(sharding_strategy)

    # init env
    seed = args.seed
    init_random_seed(seed)

    # shard models
    model_pre_and_post = qllm_model._pure_pre_and_post()
    model_shard_nums = len(sharding_strategy.keys())
    model_packs = [qllm_model.shard_model(sharding_strategy, i) for i in range(model_shard_nums)]
    # check whether the packs number equals to the sharding strategy
    assert len(model_packs) == len(sharding_strategy), "model packs number should be equal to the sharding strategy"
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # Shard on the same gpu for reference
    # move all models to cuda
    model_pre_and_post = model_pre_and_post.cuda()
    [model.decoder_layers_to_device(device) for model in model_packs]
    # move tensor to device
    batched_ids = to_device_recursive(dict(batched_ids), device)

    # print model 1, 2, 3 size in MB
    for idx, model in enumerate(model_packs):
        print("Model {} size: ".format(idx), get_model_size_cuda(model, 'MB'))
    
    # be careful that, always shards before quantization.
    # some quantizer like llm.int8 triggers when the model is run cuda() or to(device)
    # if you first move the model to cuda, then shard it, the quantizer will not work

    def generate_one_token(request_token, input_ids):
        with torch.no_grad():
            intermediate_results = request_token
            for model in model_packs:
                intermediate_results = model.decode(intermediate_results)

        request_id = intermediate_results[-1]
        # preprocessing  
        outputs = model_pre_and_post.postprocess(intermediate_results, None)
        # 2361 - 2385
        next_token_logits = outputs.logits[:, -1, :]
        next_tokens_scores = logits_processor(input_ids, next_token_logits)
        next_tokens = torch.argmax(next_tokens_scores, dim=-1)
        next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)
        new_input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
        
        return new_input_ids, next_tokens

    input_ids = batched_ids['input_ids']
    logits_processor,(unfinished_sequences, pad_token_id) = greedy_processor(qllm_model, input_ids, max_gen_tokens, max_prompt_length)
    # generate input token
    request_token = model_pre_and_post.preprocess(**batched_ids, use_cache=True, request_id=1)
    request_token2 = model_pre_and_post.preprocess(**batched_ids, use_cache=True, request_id=2)


    # init kv cache for all requests
    bs, _ = batched_ids['input_ids'].shape
    for k_model in model_packs:
        k_model.init_kv_cache(bs, max_prompt_length, max_gen_tokens, request_id=1)
        k_model.init_kv_cache(bs, max_prompt_length, max_gen_tokens, request_id=2)

    original_token = copy.deepcopy(input_ids)
    input_ids2 = copy.deepcopy(input_ids)

    for i in range(max_gen_tokens):
        new_input_ids, next_token = generate_one_token(request_token, input_ids)
        new_input_ids2, next_token2 = generate_one_token(request_token2, input_ids2)
        request_token = model_pre_and_post.preprocess_one_token(new_input_ids, next_token, attention_mask=request_token[1], use_cache=True, request_id=1)
        request_token2 = model_pre_and_post.preprocess_one_token(new_input_ids, next_token2, attention_mask=request_token2[1], use_cache=True, request_id=2)
        # print("KV Cache Size 2: ", get_iter_variable_size(model.model.decoder.kv_cache, unit='MB'))

        input_ids = new_input_ids
        input_ids2 = new_input_ids2

    result_one_time = tokenizer.batch_decode(new_input_ids, skip_special_tokens=True)
    result_one_time2 = tokenizer.batch_decode(new_input_ids2, skip_special_tokens=True)
    print("Prompts: ", prompts)
    print("Generated Token 1: ", result_one_time)
    print("Generated Token 2: ", result_one_time2)
    


    




{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5d6bd36",
   "metadata": {},
   "source": [
    "# Verify the cost model fidelity here\n",
    "- notice: the code is tested on A800, thus didn't distributed\n",
    "    - for larger model 66b, single node may not be able to hold, refer to the shard in QLLM to test the memory size to create shards among nodes.\n",
    "- if tested on smaller GPU with lower CPU capability, pls check the examples to load in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "300a2f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# sample batch size from [4, 8, 16]\n",
    "# sample prompt length from [128, 512]\n",
    "# sample maximum generated token from [100, 200]\n",
    "# sample precision from [3, 4, '8:tc-li', 16]\n",
    "batch_sizes = [2, 4, 8]\n",
    "prompt_lengths = [128, 512]\n",
    "generated_tokens = [100, 200]\n",
    "precision_candidates = [3, 4, '8:tc-li', 16]\n",
    "# set cuda device\n",
    "cuda_device_num = 3\n",
    "# torch.cuda.set_device(cuda_device_num)\n",
    "device = torch.device(f\"cuda:{cuda_device_num}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b65d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qllm.models import create_model_config\n",
    "# sample cases\n",
    "# model\n",
    "models = [\n",
    "    ['bloom', '560m'],\n",
    "    ['bloom', '1b7'],\n",
    "    ['opt', '13b'],\n",
    "    ['opt', '30b'],\n",
    "    ['opt', '66b']\n",
    "]\n",
    "model_configs = [\n",
    "    create_model_config(model_name, model_size) for (model_name, model_size) in models\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9394d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from qllm.utils import create_single_node_sharding_strategies_with_precision_specs\n",
    "def generate_configurations(model_configs):\n",
    "    configs = {}\n",
    "    for idx, config in enumerate(model_configs):\n",
    "        # get layer number\n",
    "        if hasattr(config, 'n_layer'):\n",
    "            dec_layers = config.n_layer\n",
    "        elif hasattr(config,\"num_hidden_layers\"):\n",
    "            dec_layers = config.num_hidden_layers\n",
    "        else:\n",
    "            raise ValueError(\"Not implemented\")\n",
    "        batch_size = random.choice(batch_sizes)\n",
    "        prompt_length = random.choice(prompt_lengths)\n",
    "        generated_token = random.choice(generated_tokens)\n",
    "        layer_precs = [\n",
    "            random.choice(precision_candidates) for _ in range(dec_layers)\n",
    "        ]\n",
    "        layer_precs = create_single_node_sharding_strategies_with_precision_specs(dec_layers, layer_precs)\n",
    "        \n",
    "        configs[idx] = [batch_size, prompt_length, generated_token, layer_precs]\n",
    "    \n",
    "    return configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f4e5d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from \n",
    "mem_sample_configs = generate_configurations(model_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "29ac2c30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [8,\n",
       "  512,\n",
       "  200,\n",
       "  {0: {0: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    1: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    2: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    3: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    4: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    5: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    6: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    7: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    8: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    9: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    10: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    11: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    12: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    13: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    14: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    15: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    16: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    17: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    18: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    19: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    20: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    21: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    22: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    23: {'shard': [0, 1], 'bits': [16, 16]}}}],\n",
       " 1: [4,\n",
       "  512,\n",
       "  100,\n",
       "  {0: {0: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    1: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    2: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    3: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    4: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    5: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    6: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    7: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    8: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    9: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    10: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    11: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    12: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    13: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    14: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    15: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    16: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    17: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    18: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    19: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    20: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    21: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    22: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    23: {'shard': [0, 1], 'bits': [3, 3]}}}],\n",
       " 2: [2,\n",
       "  128,\n",
       "  200,\n",
       "  {0: {0: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    1: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    2: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    3: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    4: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    5: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    6: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    7: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    8: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    9: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    10: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    11: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    12: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    13: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    14: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    15: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    16: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    17: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    18: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    19: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    20: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    21: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    22: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    23: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    24: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    25: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    26: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    27: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    28: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    29: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    30: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    31: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    32: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    33: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    34: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    35: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    36: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    37: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    38: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    39: {'shard': [0, 1], 'bits': [16, 16]}}}],\n",
       " 3: [8,\n",
       "  512,\n",
       "  200,\n",
       "  {0: {0: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    1: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    2: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    3: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    4: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    5: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    6: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    7: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    8: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    9: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    10: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    11: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    12: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    13: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    14: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    15: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    16: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    17: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    18: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    19: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    20: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    21: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    22: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    23: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    24: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    25: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    26: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    27: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    28: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    29: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    30: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    31: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    32: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    33: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    34: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    35: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    36: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    37: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    38: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    39: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    40: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    41: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    42: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    43: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    44: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    45: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    46: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    47: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']}}}],\n",
       " 4: [2,\n",
       "  128,\n",
       "  100,\n",
       "  {0: {0: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    1: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    2: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    3: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    4: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    5: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    6: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    7: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    8: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    9: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    10: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    11: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    12: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    13: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    14: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    15: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    16: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    17: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    18: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    19: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    20: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    21: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    22: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    23: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    24: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    25: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    26: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    27: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    28: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    29: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    30: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    31: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    32: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    33: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    34: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    35: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    36: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    37: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    38: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    39: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    40: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    41: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    42: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    43: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    44: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    45: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    46: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    47: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    48: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    49: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    50: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    51: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    52: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    53: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    54: {'shard': [0, 1], 'bits': [16, 16]},\n",
       "    55: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    56: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    57: {'shard': [0, 1], 'bits': [4, 4]},\n",
       "    58: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    59: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    60: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    61: {'shard': [0, 1], 'bits': ['8:tc-li', '8:tc-li']},\n",
       "    62: {'shard': [0, 1], 'bits': [3, 3]},\n",
       "    63: {'shard': [0, 1], 'bits': [4, 4]}}}]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the generated configs\n",
    "mem_sample_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04f389ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the memory occupation\n",
    "from qllm.models import create_empty_model, qllm_load_pretrained_from_size\n",
    "from qllm.utils import get_model_size_cuda, ModelMemEstimator, get_iter_variable_size\n",
    "import lptorch\n",
    "import os \n",
    "def check_mem_occupation(model_name_size_pair, model_config, mem_sample_config):\n",
    "    # create empty model is enough to estimate.\n",
    "    # load weight if you want.\n",
    "    model_name, model_size = model_name_size_pair\n",
    "    model = create_empty_model(model_name, model_size)\n",
    "#     model, tokenizer, key = qllm_load_pretrained_from_size(model_name, model_size)\n",
    "    caliber = lptorch.inner_caliber\n",
    "    caliber.set_model(model)\n",
    "    caliber.set_fake()\n",
    "    caliber.load_fake_calib_data(f'./fake_calib_{model_name}_{model_size}.pkl')\n",
    "    \n",
    "    batch_size, prompt_length, generated_token, sharding_strategy = mem_sample_config\n",
    "    # shard model\n",
    "    model_pre_and_post = model._pure_pre_and_post()\n",
    "    model = model.float() # some op need fp32 to make quantization\n",
    "    model = model.shard_model(sharding_strategy, 0) # generated shard strategy is single node\n",
    "    \n",
    "    # move model to cuda\n",
    "    model_pre_and_post = model_pre_and_post.cuda()\n",
    "    model.decoder_layers_to_device(device)\n",
    "    # init kv\n",
    "    model.init_kv_cache(batch_size, prompt_length, generated_token, request_id=1)\n",
    "\n",
    "    # get estimated size\n",
    "    if model_name == 'bloom':\n",
    "        h1 = model_config.hidden_size\n",
    "        vocab_size = model_config.vocab_size\n",
    "        max_position_embeddings = 0\n",
    "        word_embed_proj_dim = h1\n",
    "        h2 = h1 * 4\n",
    "        \n",
    "    elif model_name == 'opt':\n",
    "        h1 = model_config.hidden_size\n",
    "        h2 = model_config.ffn_dim\n",
    "        vocab_size = model_config.vocab_size\n",
    "        max_position_embeddings = model_config.max_position_embeddings\n",
    "        word_embed_proj_dim = model_config.word_embed_proj_dim\n",
    "        \n",
    "    model_mem_estimator = ModelMemEstimator(h1, h2, batch_size, prompt_length, generated_token, \\\n",
    "                                            vocab_size, max_position_embeddings, word_embed_proj_dim)\n",
    "    \n",
    "    # comparison\n",
    "    overall_real_mem = 0\n",
    "    overall_est_mem = 0\n",
    "    # data rank\n",
    "    model_pre_post_real = get_model_size_cuda(model_pre_and_post, 'MB')[0] + get_model_size_cuda(model_pre_and_post.lm_head, 'MB')[0]\n",
    "    model_pre_post_estimate = model_mem_estimator.calculate_prepost_mem(unit='MB')[0]\n",
    "    print(f\"Model pre and post size: {model_pre_post_real} MB\")\n",
    "    print(f\"Est Model pre and post size: {model_pre_post_estimate} MB\")\n",
    "    overall_real_mem += model_pre_post_real\n",
    "    overall_est_mem += model_pre_post_estimate\n",
    "    # model\n",
    "    model_mem_real = get_model_size_cuda(model, 'MB')[0]\n",
    "    model_mem_est = model_mem_estimator.calculate_model_occupation_of_partition(sharding_strategy[0], unit='MB')[0]\n",
    "    print(f\"Model size: {model_mem_real} MB\")\n",
    "    print(f\"Estimated Model {model_mem_est} MB\")\n",
    "    overall_real_mem += model_mem_real\n",
    "    overall_est_mem += model_mem_est\n",
    "    # KV size\n",
    "    request_num = 1 # allow more request nums \n",
    "    kv_mem_est = kv_mem_real = 0\n",
    "    if model_name == 'bloom':\n",
    "        kv_mem_real = get_iter_variable_size(model.transformer.get_all_kv_cache_dict(), unit='MB')\n",
    "        kv_mem_est = request_num * model_mem_estimator.calculate_kv_occupation_of_partition(sharding_strategy[0], 'MB')[0]\n",
    "\n",
    "    elif model_name == 'opt':\n",
    "        kv_mem_real = get_iter_variable_size(model.model.decoder.get_all_kv_cache_dict(), unit='MB')\n",
    "        kv_mem_est = request_num * model_mem_estimator.calculate_kv_occupation_of_partition(sharding_strategy[0], 'MB')[0]\n",
    "    overall_real_mem += kv_mem_real\n",
    "    overall_est_mem += kv_mem_est\n",
    "    print(f\"Model KV size: {kv_mem_real}MB\")\n",
    "    print(f\"Estimated Model KV: {kv_mem_est}MB\")\n",
    "    print(f\"Overall mem: Real {overall_real_mem} Vs. Est {overall_est_mem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd9679c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mem_est(check_idx):\n",
    "    model_name_size_pair = models[check_idx]\n",
    "    model_config = model_configs[check_idx]\n",
    "    mem_sample_config = mem_sample_configs[check_idx]\n",
    "    check_mem_occupation(model_name_size_pair, model_config, mem_sample_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9ea1b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "# Generate some fake calibaration data \n",
    "!python3 fake_calib_sample.py --model-name bloom --model-size 560m\n",
    "!python3 fake_calib_sample.py --model-name bloom --model-size 1b7\n",
    "!python3 fake_calib_sample.py --model-name opt --model-size 13b\n",
    "!python3 fake_calib_sample.py --model-name opt --model-size 30b\n",
    "!python3 fake_calib_sample.py --model-name opt --model-size 66b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41f65726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pre and post size: 980.0078125 MB\n",
      "Est Model pre and post size: 980.00390625 MB\n",
      "Model size: 320.98809814453125 MB\n",
      "Estimated Model 319.59375 MB\n",
      "Model KV size: 534.0MB\n",
      "Estimated Model KV: 534.0MB\n",
      "Overall mem: Real 1834.9959106445312 Vs. Est 1833.59765625\n"
     ]
    }
   ],
   "source": [
    "# select the check_idx\n",
    "check_idx = 0\n",
    "check_mem_est(check_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dbbefe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pre and post size: 1960.015625 MB\n",
      "Est Model pre and post size: 1960.0078125 MB\n",
      "Model size: 1011.1935424804688 MB\n",
      "Estimated Model 1008.1875 MB\n",
      "Model KV size: 459.0MB\n",
      "Estimated Model KV: 459.0MB\n",
      "Overall mem: Real 3430.2091674804688 Vs. Est 3427.1953125\n"
     ]
    }
   ],
   "source": [
    "check_idx = 1\n",
    "check_mem_est(check_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

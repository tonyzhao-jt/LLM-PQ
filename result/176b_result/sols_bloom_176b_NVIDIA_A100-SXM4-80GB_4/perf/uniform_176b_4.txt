
libgomp: Invalid value for environment variable OMP_NUM_THREADS

libgomp: Invalid value for environment variable OMP_NUM_THREADS

libgomp: Invalid value for environment variable OMP_NUM_THREADS

libgomp: Invalid value for environment variable OMP_NUM_THREADS

libgomp: Invalid value for environment variable OMP_NUM_THREADS

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/tiger/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
bin /home/tiger/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.0
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/tiger/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.0
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/tiger/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
bin /home/tiger/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
bin /home/tiger/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 8.0
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/tiger/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 8.0
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /home/tiger/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...

libgomp: Invalid value for environment variable OMP_NUM_THREADS

libgomp: Invalid value for environment variable OMP_NUM_THREADS

libgomp: Invalid value for environment variable OMP_NUM_THREADS

libgomp: Invalid value for environment variable OMP_NUM_THREADS
rank 2 is in stage 2
rank 1 is in stage 1
Pipeline Data Loaded, with prefill cnts:  4
rank 0 is in stage 0
rank 3 is in stage 3
create layer: 36|create layer: 18|create layer: 0|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 53|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 37|create layer: 19|create layer: 1|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 54|create layer: 20|create layer: 38|create layer: 2|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 55|create layer: 39|create layer: 21|create layer: 3|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 56|create layer: 40|create layer: 22|create layer: 4|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 57|create layer: 23|create layer: 41|create layer: 5|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 58|create layer: 24|create layer: 42|create layer: 6|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 59|create layer: 25|create layer: 43|create layer: 7|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 60|create layer: 26|create layer: 44|create layer: 8|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 61|create layer: 27|create layer: 45|create layer: 9|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 62|create layer: 28|create layer: 46|create layer: 10|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 63|create layer: 29|create layer: 47|create layer: 11|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 64|create layer: 30|create layer: 12|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 48|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 65|create layer: 31|create layer: 13|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 49|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 66|create layer: 32|create layer: 14|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 50|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 67|create layer: 33|create layer: 15|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 51|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 68|create layer: 34|create layer: 16|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
create layer: 52|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Stage 2 module sharded
Stage 2 kv initialized
create layer: 69|Stage 3 module sharded
Stage 3 kv initialized
create layer: 35|Stage 1 module sharded
Stage 1 kv initialized
create layer: 17|huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Stage 0 module sharded
Stage 0 kv initialized
rank_src: 3, rank_dst: 1, rank: 0, results_cb: <function handle_results at 0x7fb6cf1dbaf0>rank_src: 0, rank_dst: 2, rank: 1, results_cb: None
rank_src: 1, rank_dst: 3, rank: 2, results_cb: None

rank_src: 2, rank_dst: 0, rank: 3, results_cb: None
2023-05-29 02:26:53,368 - INFO - start pipe data
/home/tiger/.local/lib/python3.9/site-packages/qpipe/p2p/util.py:32: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  byte_storage = torch.ByteStorage.from_buffer(bytes_io.getvalue())  # type: ignore[attr-defined]
/home/tiger/.local/lib/python3.9/site-packages/qpipe/p2p/util.py:32: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  byte_storage = torch.ByteStorage.from_buffer(bytes_io.getvalue())  # type: ignore[attr-defined]
/home/tiger/.local/lib/python3.9/site-packages/qpipe/p2p/util.py:32: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  byte_storage = torch.ByteStorage.from_buffer(bytes_io.getvalue())  # type: ignore[attr-defined]
/home/tiger/.local/lib/python3.9/site-packages/qpipe/p2p/util.py:32: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  byte_storage = torch.ByteStorage.from_buffer(bytes_io.getvalue())  # type: ignore[attr-defined]
2023-05-29 02:34:23,836 - INFO - Request id 0 done for token 1
2023-05-29 02:35:59,948 - INFO - Request id 1 done for token 1
2023-05-29 02:37:53,574 - INFO - Request id 2 done for token 1
2023-05-29 02:39:45,476 - INFO - Request id 3 done for token 1
2023-05-29 02:39:46,497 - INFO - Latency is 773.128789, throughput is 0.041390
2023-05-29 02:39:46,497 - INFO - Token throughput is 0.082781
2023-05-29 02:39:56,509 - INFO - start pipe data
2023-05-29 02:47:26,393 - INFO - Request id 0 done for token 1
2023-05-29 02:49:02,481 - INFO - Request id 1 done for token 1
2023-05-29 02:50:56,048 - INFO - Request id 2 done for token 1
2023-05-29 02:52:47,881 - INFO - Request id 3 done for token 1
2023-05-29 02:52:48,009 - INFO - Request id 0 done for token 2
2023-05-29 02:52:48,233 - INFO - Request id 1 done for token 2
2023-05-29 02:52:48,477 - INFO - Request id 2 done for token 2
2023-05-29 02:52:48,924 - INFO - Request id 3 done for token 2
2023-05-29 02:52:49,180 - INFO - Request id 0 done for token 3
2023-05-29 02:52:49,436 - INFO - Request id 1 done for token 3
2023-05-29 02:52:49,703 - INFO - Request id 2 done for token 3
2023-05-29 02:52:49,959 - INFO - Request id 3 done for token 3
2023-05-29 02:52:50,223 - INFO - Request id 0 done for token 4
2023-05-29 02:52:50,482 - INFO - Request id 1 done for token 4
2023-05-29 02:52:50,744 - INFO - Request id 2 done for token 4
2023-05-29 02:52:51,005 - INFO - Request id 3 done for token 4
2023-05-29 02:52:51,266 - INFO - Request id 0 done for token 5
2023-05-29 02:52:51,526 - INFO - Request id 1 done for token 5
2023-05-29 02:52:51,785 - INFO - Request id 2 done for token 5
2023-05-29 02:52:52,042 - INFO - Request id 3 done for token 5
2023-05-29 02:52:52,298 - INFO - Request id 0 done for token 6
2023-05-29 02:52:52,557 - INFO - Request id 1 done for token 6
2023-05-29 02:52:52,816 - INFO - Request id 2 done for token 6
2023-05-29 02:52:53,071 - INFO - Request id 3 done for token 6
2023-05-29 02:52:53,327 - INFO - Request id 0 done for token 7
2023-05-29 02:52:53,589 - INFO - Request id 1 done for token 7
2023-05-29 02:52:53,846 - INFO - Request id 2 done for token 7
2023-05-29 02:52:54,105 - INFO - Request id 3 done for token 7
2023-05-29 02:52:54,360 - INFO - Request id 0 done for token 8
2023-05-29 02:52:54,616 - INFO - Request id 1 done for token 8
2023-05-29 02:52:54,874 - INFO - Request id 2 done for token 8
2023-05-29 02:52:55,139 - INFO - Request id 3 done for token 8
2023-05-29 02:52:55,398 - INFO - Request id 0 done for token 9
2023-05-29 02:52:55,657 - INFO - Request id 1 done for token 9
2023-05-29 02:52:55,917 - INFO - Request id 2 done for token 9
2023-05-29 02:52:56,178 - INFO - Request id 3 done for token 9
2023-05-29 02:52:56,439 - INFO - Request id 0 done for token 10
2023-05-29 02:52:56,697 - INFO - Request id 1 done for token 10
2023-05-29 02:52:56,956 - INFO - Request id 2 done for token 10
2023-05-29 02:52:57,217 - INFO - Request id 3 done for token 10
2023-05-29 02:52:57,472 - INFO - Request id 0 done for token 11
2023-05-29 02:52:57,729 - INFO - Request id 1 done for token 11
2023-05-29 02:52:57,984 - INFO - Request id 2 done for token 11
2023-05-29 02:52:58,239 - INFO - Request id 3 done for token 11
2023-05-29 02:52:58,499 - INFO - Request id 0 done for token 12
2023-05-29 02:52:58,755 - INFO - Request id 1 done for token 12
2023-05-29 02:52:59,018 - INFO - Request id 2 done for token 12
2023-05-29 02:52:59,279 - INFO - Request id 3 done for token 12
2023-05-29 02:52:59,538 - INFO - Request id 0 done for token 13
2023-05-29 02:52:59,797 - INFO - Request id 1 done for token 13
2023-05-29 02:53:00,058 - INFO - Request id 2 done for token 13
2023-05-29 02:53:00,318 - INFO - Request id 3 done for token 13
2023-05-29 02:53:00,579 - INFO - Request id 0 done for token 14
2023-05-29 02:53:00,840 - INFO - Request id 1 done for token 14
2023-05-29 02:53:01,102 - INFO - Request id 2 done for token 14
2023-05-29 02:53:01,364 - INFO - Request id 3 done for token 14
2023-05-29 02:53:01,624 - INFO - Request id 0 done for token 15
2023-05-29 02:53:01,884 - INFO - Request id 1 done for token 15
2023-05-29 02:53:02,145 - INFO - Request id 2 done for token 15
2023-05-29 02:53:02,403 - INFO - Request id 3 done for token 15
2023-05-29 02:53:02,662 - INFO - Request id 0 done for token 16
2023-05-29 02:53:02,919 - INFO - Request id 1 done for token 16
2023-05-29 02:53:03,177 - INFO - Request id 2 done for token 16
2023-05-29 02:53:03,435 - INFO - Request id 3 done for token 16
2023-05-29 02:53:03,694 - INFO - Request id 0 done for token 17
2023-05-29 02:53:03,950 - INFO - Request id 1 done for token 17
2023-05-29 02:53:04,208 - INFO - Request id 2 done for token 17
2023-05-29 02:53:04,464 - INFO - Request id 3 done for token 17
2023-05-29 02:53:04,720 - INFO - Request id 0 done for token 18
2023-05-29 02:53:04,977 - INFO - Request id 1 done for token 18
2023-05-29 02:53:05,237 - INFO - Request id 2 done for token 18
2023-05-29 02:53:05,493 - INFO - Request id 3 done for token 18
2023-05-29 02:53:05,752 - INFO - Request id 0 done for token 19
2023-05-29 02:53:06,011 - INFO - Request id 1 done for token 19
2023-05-29 02:53:06,267 - INFO - Request id 2 done for token 19
2023-05-29 02:53:06,526 - INFO - Request id 3 done for token 19
2023-05-29 02:53:06,785 - INFO - Request id 0 done for token 20
2023-05-29 02:53:07,041 - INFO - Request id 1 done for token 20
2023-05-29 02:53:07,300 - INFO - Request id 2 done for token 20
2023-05-29 02:53:07,557 - INFO - Request id 3 done for token 20
2023-05-29 02:53:07,820 - INFO - Request id 0 done for token 21
2023-05-29 02:53:08,077 - INFO - Request id 1 done for token 21
2023-05-29 02:53:08,334 - INFO - Request id 2 done for token 21
2023-05-29 02:53:08,590 - INFO - Request id 3 done for token 21
2023-05-29 02:53:08,848 - INFO - Request id 0 done for token 22
2023-05-29 02:53:09,109 - INFO - Request id 1 done for token 22
2023-05-29 02:53:09,369 - INFO - Request id 2 done for token 22
2023-05-29 02:53:09,625 - INFO - Request id 3 done for token 22
2023-05-29 02:53:09,881 - INFO - Request id 0 done for token 23
2023-05-29 02:53:10,138 - INFO - Request id 1 done for token 23
2023-05-29 02:53:10,397 - INFO - Request id 2 done for token 23
2023-05-29 02:53:10,659 - INFO - Request id 3 done for token 23
2023-05-29 02:53:10,930 - INFO - Request id 0 done for token 24
2023-05-29 02:53:11,189 - INFO - Request id 1 done for token 24
2023-05-29 02:53:11,447 - INFO - Request id 2 done for token 24
2023-05-29 02:53:11,706 - INFO - Request id 3 done for token 24
2023-05-29 02:53:11,962 - INFO - Request id 0 done for token 25
2023-05-29 02:53:12,218 - INFO - Request id 1 done for token 25
2023-05-29 02:53:12,478 - INFO - Request id 2 done for token 25
2023-05-29 02:53:12,734 - INFO - Request id 3 done for token 25
2023-05-29 02:53:12,997 - INFO - Request id 0 done for token 26
2023-05-29 02:53:13,255 - INFO - Request id 1 done for token 26
2023-05-29 02:53:13,513 - INFO - Request id 2 done for token 26
2023-05-29 02:53:13,769 - INFO - Request id 3 done for token 26
2023-05-29 02:53:14,028 - INFO - Request id 0 done for token 27
2023-05-29 02:53:14,284 - INFO - Request id 1 done for token 27
2023-05-29 02:53:14,543 - INFO - Request id 2 done for token 27
2023-05-29 02:53:14,800 - INFO - Request id 3 done for token 27
2023-05-29 02:53:15,056 - INFO - Request id 0 done for token 28
2023-05-29 02:53:15,317 - INFO - Request id 1 done for token 28
2023-05-29 02:53:15,573 - INFO - Request id 2 done for token 28
2023-05-29 02:53:15,831 - INFO - Request id 3 done for token 28
2023-05-29 02:53:16,090 - INFO - Request id 0 done for token 29
2023-05-29 02:53:16,350 - INFO - Request id 1 done for token 29
2023-05-29 02:53:16,615 - INFO - Request id 2 done for token 29
2023-05-29 02:53:16,875 - INFO - Request id 3 done for token 29
2023-05-29 02:53:17,135 - INFO - Request id 0 done for token 30
2023-05-29 02:53:17,395 - INFO - Request id 1 done for token 30
2023-05-29 02:53:17,657 - INFO - Request id 2 done for token 30
2023-05-29 02:53:17,918 - INFO - Request id 3 done for token 30
2023-05-29 02:53:18,179 - INFO - Request id 0 done for token 31
2023-05-29 02:53:18,439 - INFO - Request id 1 done for token 31
2023-05-29 02:53:18,700 - INFO - Request id 2 done for token 31
2023-05-29 02:53:18,961 - INFO - Request id 3 done for token 31
2023-05-29 02:53:19,221 - INFO - Request id 0 done for token 32
2023-05-29 02:53:19,479 - INFO - Request id 1 done for token 32
2023-05-29 02:53:19,736 - INFO - Request id 2 done for token 32
2023-05-29 02:53:19,994 - INFO - Request id 3 done for token 32
2023-05-29 02:53:20,263 - INFO - Request id 0 done for token 33
2023-05-29 02:53:20,523 - INFO - Request id 1 done for token 33
2023-05-29 02:53:20,783 - INFO - Request id 2 done for token 33
2023-05-29 02:53:21,046 - INFO - Request id 3 done for token 33
2023-05-29 02:53:21,306 - INFO - Request id 0 done for token 34
2023-05-29 02:53:21,566 - INFO - Request id 1 done for token 34
2023-05-29 02:53:21,831 - INFO - Request id 2 done for token 34
2023-05-29 02:53:22,092 - INFO - Request id 3 done for token 34
2023-05-29 02:53:22,354 - INFO - Request id 0 done for token 35
2023-05-29 02:53:22,618 - INFO - Request id 1 done for token 35
2023-05-29 02:53:22,879 - INFO - Request id 2 done for token 35
2023-05-29 02:53:23,140 - INFO - Request id 3 done for token 35
2023-05-29 02:53:23,402 - INFO - Request id 0 done for token 36
2023-05-29 02:53:23,666 - INFO - Request id 1 done for token 36
2023-05-29 02:53:23,930 - INFO - Request id 2 done for token 36
2023-05-29 02:53:24,192 - INFO - Request id 3 done for token 36
2023-05-29 02:53:24,457 - INFO - Request id 0 done for token 37
2023-05-29 02:53:24,722 - INFO - Request id 1 done for token 37
2023-05-29 02:53:24,984 - INFO - Request id 2 done for token 37
2023-05-29 02:53:25,246 - INFO - Request id 3 done for token 37
2023-05-29 02:53:25,510 - INFO - Request id 0 done for token 38
2023-05-29 02:53:25,773 - INFO - Request id 1 done for token 38
2023-05-29 02:53:26,036 - INFO - Request id 2 done for token 38
2023-05-29 02:53:26,298 - INFO - Request id 3 done for token 38
2023-05-29 02:53:26,561 - INFO - Request id 0 done for token 39
2023-05-29 02:53:26,822 - INFO - Request id 1 done for token 39
2023-05-29 02:53:27,084 - INFO - Request id 2 done for token 39
2023-05-29 02:53:27,346 - INFO - Request id 3 done for token 39
2023-05-29 02:53:27,606 - INFO - Request id 0 done for token 40
2023-05-29 02:53:27,867 - INFO - Request id 1 done for token 40
2023-05-29 02:53:28,128 - INFO - Request id 2 done for token 40
2023-05-29 02:53:28,389 - INFO - Request id 3 done for token 40
2023-05-29 02:53:28,650 - INFO - Request id 0 done for token 41
2023-05-29 02:53:28,911 - INFO - Request id 1 done for token 41
2023-05-29 02:53:29,173 - INFO - Request id 2 done for token 41
2023-05-29 02:53:29,432 - INFO - Request id 3 done for token 41
2023-05-29 02:53:29,699 - INFO - Request id 0 done for token 42
2023-05-29 02:53:29,961 - INFO - Request id 1 done for token 42
2023-05-29 02:53:30,224 - INFO - Request id 2 done for token 42
2023-05-29 02:53:30,486 - INFO - Request id 3 done for token 42
2023-05-29 02:53:30,748 - INFO - Request id 0 done for token 43
2023-05-29 02:53:31,011 - INFO - Request id 1 done for token 43
2023-05-29 02:53:31,272 - INFO - Request id 2 done for token 43
2023-05-29 02:53:31,534 - INFO - Request id 3 done for token 43
2023-05-29 02:53:31,796 - INFO - Request id 0 done for token 44
2023-05-29 02:53:32,061 - INFO - Request id 1 done for token 44
2023-05-29 02:53:32,324 - INFO - Request id 2 done for token 44
2023-05-29 02:53:32,586 - INFO - Request id 3 done for token 44
2023-05-29 02:53:32,849 - INFO - Request id 0 done for token 45
2023-05-29 02:53:33,112 - INFO - Request id 1 done for token 45
2023-05-29 02:53:33,373 - INFO - Request id 2 done for token 45
2023-05-29 02:53:33,637 - INFO - Request id 3 done for token 45
2023-05-29 02:53:33,900 - INFO - Request id 0 done for token 46
2023-05-29 02:53:34,163 - INFO - Request id 1 done for token 46
2023-05-29 02:53:34,427 - INFO - Request id 2 done for token 46
2023-05-29 02:53:34,687 - INFO - Request id 3 done for token 46
2023-05-29 02:53:34,954 - INFO - Request id 0 done for token 47
2023-05-29 02:53:35,217 - INFO - Request id 1 done for token 47
2023-05-29 02:53:35,480 - INFO - Request id 2 done for token 47
2023-05-29 02:53:35,742 - INFO - Request id 3 done for token 47
2023-05-29 02:53:36,007 - INFO - Request id 0 done for token 48
2023-05-29 02:53:36,269 - INFO - Request id 1 done for token 48
2023-05-29 02:53:36,532 - INFO - Request id 2 done for token 48
2023-05-29 02:53:36,793 - INFO - Request id 3 done for token 48
2023-05-29 02:53:37,057 - INFO - Request id 0 done for token 49
2023-05-29 02:53:37,316 - INFO - Request id 1 done for token 49
2023-05-29 02:53:37,576 - INFO - Request id 2 done for token 49
2023-05-29 02:53:37,841 - INFO - Request id 3 done for token 49
2023-05-29 02:53:38,101 - INFO - Request id 0 done for token 50
2023-05-29 02:53:38,364 - INFO - Request id 1 done for token 50
2023-05-29 02:53:38,627 - INFO - Request id 2 done for token 50
2023-05-29 02:53:38,891 - INFO - Request id 3 done for token 50
2023-05-29 02:53:39,155 - INFO - Request id 0 done for token 51
2023-05-29 02:53:39,418 - INFO - Request id 1 done for token 51
2023-05-29 02:53:39,681 - INFO - Request id 2 done for token 51
2023-05-29 02:53:39,946 - INFO - Request id 3 done for token 51
2023-05-29 02:53:40,208 - INFO - Request id 0 done for token 52
2023-05-29 02:53:40,473 - INFO - Request id 1 done for token 52
2023-05-29 02:53:40,737 - INFO - Request id 2 done for token 52
2023-05-29 02:53:41,001 - INFO - Request id 3 done for token 52
2023-05-29 02:53:41,265 - INFO - Request id 0 done for token 53
2023-05-29 02:53:41,529 - INFO - Request id 1 done for token 53
2023-05-29 02:53:41,792 - INFO - Request id 2 done for token 53
2023-05-29 02:53:42,054 - INFO - Request id 3 done for token 53
2023-05-29 02:53:42,317 - INFO - Request id 0 done for token 54
2023-05-29 02:53:42,580 - INFO - Request id 1 done for token 54
2023-05-29 02:53:42,843 - INFO - Request id 2 done for token 54
2023-05-29 02:53:43,107 - INFO - Request id 3 done for token 54
2023-05-29 02:53:43,371 - INFO - Request id 0 done for token 55
2023-05-29 02:53:43,636 - INFO - Request id 1 done for token 55
2023-05-29 02:53:43,901 - INFO - Request id 2 done for token 55
2023-05-29 02:53:44,166 - INFO - Request id 3 done for token 55
2023-05-29 02:53:44,430 - INFO - Request id 0 done for token 56
2023-05-29 02:53:44,692 - INFO - Request id 1 done for token 56
2023-05-29 02:53:44,954 - INFO - Request id 2 done for token 56
2023-05-29 02:53:45,218 - INFO - Request id 3 done for token 56
2023-05-29 02:53:45,480 - INFO - Request id 0 done for token 57
2023-05-29 02:53:45,744 - INFO - Request id 1 done for token 57
2023-05-29 02:53:46,008 - INFO - Request id 2 done for token 57
2023-05-29 02:53:46,271 - INFO - Request id 3 done for token 57
2023-05-29 02:53:46,534 - INFO - Request id 0 done for token 58
2023-05-29 02:53:46,798 - INFO - Request id 1 done for token 58
2023-05-29 02:53:47,061 - INFO - Request id 2 done for token 58
2023-05-29 02:53:47,325 - INFO - Request id 3 done for token 58
2023-05-29 02:53:47,588 - INFO - Request id 0 done for token 59
2023-05-29 02:53:47,851 - INFO - Request id 1 done for token 59
2023-05-29 02:53:48,115 - INFO - Request id 2 done for token 59
2023-05-29 02:53:48,380 - INFO - Request id 3 done for token 59
2023-05-29 02:53:48,644 - INFO - Request id 0 done for token 60
2023-05-29 02:53:48,907 - INFO - Request id 1 done for token 60
2023-05-29 02:53:49,171 - INFO - Request id 2 done for token 60
2023-05-29 02:53:49,434 - INFO - Request id 3 done for token 60
2023-05-29 02:53:49,698 - INFO - Request id 0 done for token 61
2023-05-29 02:53:49,962 - INFO - Request id 1 done for token 61
2023-05-29 02:53:50,225 - INFO - Request id 2 done for token 61
2023-05-29 02:53:50,489 - INFO - Request id 3 done for token 61
2023-05-29 02:53:50,753 - INFO - Request id 0 done for token 62
2023-05-29 02:53:51,016 - INFO - Request id 1 done for token 62
2023-05-29 02:53:51,279 - INFO - Request id 2 done for token 62
2023-05-29 02:53:51,543 - INFO - Request id 3 done for token 62
2023-05-29 02:53:51,808 - INFO - Request id 0 done for token 63
2023-05-29 02:53:52,073 - INFO - Request id 1 done for token 63
2023-05-29 02:53:52,337 - INFO - Request id 2 done for token 63
2023-05-29 02:53:52,601 - INFO - Request id 3 done for token 63
2023-05-29 02:53:52,868 - INFO - Request id 0 done for token 64
2023-05-29 02:53:53,131 - INFO - Request id 1 done for token 64
2023-05-29 02:53:53,392 - INFO - Request id 2 done for token 64
2023-05-29 02:53:53,660 - INFO - Request id 3 done for token 64
2023-05-29 02:53:53,925 - INFO - Request id 0 done for token 65
2023-05-29 02:53:54,189 - INFO - Request id 1 done for token 65
2023-05-29 02:53:54,454 - INFO - Request id 2 done for token 65
2023-05-29 02:53:54,720 - INFO - Request id 3 done for token 65
2023-05-29 02:53:54,984 - INFO - Request id 0 done for token 66
2023-05-29 02:53:55,245 - INFO - Request id 1 done for token 66
2023-05-29 02:53:55,513 - INFO - Request id 2 done for token 66
2023-05-29 02:53:55,777 - INFO - Request id 3 done for token 66
2023-05-29 02:53:56,043 - INFO - Request id 0 done for token 67
2023-05-29 02:53:56,309 - INFO - Request id 1 done for token 67
2023-05-29 02:53:56,573 - INFO - Request id 2 done for token 67
2023-05-29 02:53:56,837 - INFO - Request id 3 done for token 67
2023-05-29 02:53:57,102 - INFO - Request id 0 done for token 68
2023-05-29 02:53:57,365 - INFO - Request id 1 done for token 68
2023-05-29 02:53:57,630 - INFO - Request id 2 done for token 68
2023-05-29 02:53:57,895 - INFO - Request id 3 done for token 68
2023-05-29 02:53:58,160 - INFO - Request id 0 done for token 69
2023-05-29 02:53:58,425 - INFO - Request id 1 done for token 69
2023-05-29 02:53:58,690 - INFO - Request id 2 done for token 69
2023-05-29 02:53:58,954 - INFO - Request id 3 done for token 69
2023-05-29 02:53:59,215 - INFO - Request id 0 done for token 70
2023-05-29 02:53:59,482 - INFO - Request id 1 done for token 70
2023-05-29 02:53:59,748 - INFO - Request id 2 done for token 70
2023-05-29 02:54:00,015 - INFO - Request id 3 done for token 70
2023-05-29 02:54:00,280 - INFO - Request id 0 done for token 71
2023-05-29 02:54:00,545 - INFO - Request id 1 done for token 71
2023-05-29 02:54:00,812 - INFO - Request id 2 done for token 71
2023-05-29 02:54:01,077 - INFO - Request id 3 done for token 71
2023-05-29 02:54:01,342 - INFO - Request id 0 done for token 72
2023-05-29 02:54:01,607 - INFO - Request id 1 done for token 72
2023-05-29 02:54:01,872 - INFO - Request id 2 done for token 72
2023-05-29 02:54:02,138 - INFO - Request id 3 done for token 72
2023-05-29 02:54:02,404 - INFO - Request id 0 done for token 73
2023-05-29 02:54:02,672 - INFO - Request id 1 done for token 73
2023-05-29 02:54:02,938 - INFO - Request id 2 done for token 73
2023-05-29 02:54:03,205 - INFO - Request id 3 done for token 73
2023-05-29 02:54:03,472 - INFO - Request id 0 done for token 74
2023-05-29 02:54:03,738 - INFO - Request id 1 done for token 74
2023-05-29 02:54:04,007 - INFO - Request id 2 done for token 74
2023-05-29 02:54:04,273 - INFO - Request id 3 done for token 74
2023-05-29 02:54:04,545 - INFO - Request id 0 done for token 75
2023-05-29 02:54:04,810 - INFO - Request id 1 done for token 75
2023-05-29 02:54:05,076 - INFO - Request id 2 done for token 75
2023-05-29 02:54:05,341 - INFO - Request id 3 done for token 75
2023-05-29 02:54:05,611 - INFO - Request id 0 done for token 76
2023-05-29 02:54:05,877 - INFO - Request id 1 done for token 76
2023-05-29 02:54:06,144 - INFO - Request id 2 done for token 76
2023-05-29 02:54:06,407 - INFO - Request id 3 done for token 76
2023-05-29 02:54:06,676 - INFO - Request id 0 done for token 77
2023-05-29 02:54:06,943 - INFO - Request id 1 done for token 77
2023-05-29 02:54:07,211 - INFO - Request id 2 done for token 77
2023-05-29 02:54:07,509 - INFO - Request id 3 done for token 77
2023-05-29 02:54:07,771 - INFO - Request id 0 done for token 78
2023-05-29 02:54:08,037 - INFO - Request id 1 done for token 78
2023-05-29 02:54:08,307 - INFO - Request id 2 done for token 78
2023-05-29 02:54:08,570 - INFO - Request id 3 done for token 78
2023-05-29 02:54:08,835 - INFO - Request id 0 done for token 79
2023-05-29 02:54:09,101 - INFO - Request id 1 done for token 79
2023-05-29 02:54:09,367 - INFO - Request id 2 done for token 79
2023-05-29 02:54:09,633 - INFO - Request id 3 done for token 79
2023-05-29 02:54:10,666 - INFO - Latency is 854.157313, throughput is 0.037464
2023-05-29 02:54:10,666 - INFO - Token throughput is 2.997106
2023-05-29 02:54:10,712 - INFO - handle_cmd: stop
2023-05-29 02:54:10,713 - INFO - handle_cmd: stop
2023-05-29 02:54:10,767 - INFO - handle_cmd: stop
 Traceback (most recent call last) 
 /opt/tiger/launch/qsync/QPipe/tests/main_p2p.py:329 in <module>                                  
                                                                                                  
   326       model_pre_and_post = model_pre_and_post.cuda()                                     
   327                                                                                           
   328    run_pipeline_p2p(loaded_llm_cpu, dist_cfg, sharding_strategy=sharding_strategy)        
  329    simple_queue_thread.join()                                                             
   330    # from torch import profiler                                                           
   331    # from torch.profiler import profile, record_function, ProfilerActivity                
   332    # with profiler.profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], wi   

AttributeError: 'NoneType' object has no attribute 'join'
 Traceback (most recent call last) 
 /opt/tiger/launch/qsync/QPipe/tests/main_p2p.py:329 in <module>                                  
                                                                                                  
   326       model_pre_and_post = model_pre_and_post.cuda()                                     
   327                                                                                           
   328    run_pipeline_p2p(loaded_llm_cpu, dist_cfg, sharding_strategy=sharding_strategy)        
  329    simple_queue_thread.join()                                                             
   330    # from torch import profiler                                                           
   331    # from torch.profiler import profile, record_function, ProfilerActivity                
   332    # with profiler.profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], wi   

AttributeError: 'NoneType' object has no attribute 'join'
 Traceback (most recent call last) 
 /opt/tiger/launch/qsync/QPipe/tests/main_p2p.py:329 in <module>                                  
                                                                                                  
   326       model_pre_and_post = model_pre_and_post.cuda()                                     
   327                                                                                           
   328    run_pipeline_p2p(loaded_llm_cpu, dist_cfg, sharding_strategy=sharding_strategy)        
  329    simple_queue_thread.join()                                                             
   330    # from torch import profiler                                                           
   331    # from torch.profiler import profile, record_function, ProfilerActivity                
   332    # with profiler.profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], wi   

AttributeError: 'NoneType' object has no attribute 'join'
Exception in thread Thread-337:
Traceback (most recent call last):
  File "/usr/lib/python3.9/threading.py", line 954, in _bootstrap_inner
    self.run()
  File "/home/tiger/.local/lib/python3.9/site-packages/qpipe/p2p/util.py", line 24, in run
    self._req.wait()
RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [10.176.100.32]:50610
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 36808 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 36809) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_p2p.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-05-29_02:54:13
  host      : n176-100-032.byted.org
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 36810)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-05-29_02:54:13
  host      : n176-100-032.byted.org
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 36811)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-05-29_02:54:13
  host      : n176-100-032.byted.org
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 36809)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
